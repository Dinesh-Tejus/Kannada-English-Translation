{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d962c41-9c1b-4d98-8f03-36ca6a912d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.0.1 in ./.local/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: networkx in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (2.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (1.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: jinja2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (2.11.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: filelock in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: wheel in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.36.2)\n",
      "Requirement already satisfied: setuptools in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1) (3.29.2)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1) (18.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch==2.0.1) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch==2.0.1) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch==2.0.1) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b46f81-36f3-48fd-b5c2-c50fd1f2309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: indic_nlp_library==0.92 in ./.local/lib/python3.8/site-packages (0.92)\n",
      "Requirement already satisfied: pandas in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from indic_nlp_library==0.92) (1.2.4)\n",
      "Requirement already satisfied: sphinx-argparse in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (0.4.0)\n",
      "Requirement already satisfied: morfessor in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (2.0.6)\n",
      "Requirement already satisfied: sphinx-rtd-theme in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (2.0.0)\n",
      "Requirement already satisfied: numpy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from indic_nlp_library==0.92) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from pandas->indic_nlp_library==0.92) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from pandas->indic_nlp_library==0.92) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->indic_nlp_library==0.92) (1.15.0)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in ./.local/lib/python3.8/site-packages (from sphinx-argparse->indic_nlp_library==0.92) (5.1.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.3)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (7.1.0)\n",
      "Requirement already satisfied: docutils<0.20,>=0.14 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (0.17.1)\n",
      "Requirement already satisfied: imagesize in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.2.0)\n",
      "Requirement already satisfied: Pygments>=2.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.8.1)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.11.3)\n",
      "Requirement already satisfied: babel>=1.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.9.0)\n",
      "Requirement already satisfied: packaging in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (20.9)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.2)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.2)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.5.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.25.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from importlib-metadata>=4.4->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.26.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.4.7)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in ./.local/lib/python3.8/site-packages (from sphinx-rtd-theme->indic_nlp_library==0.92) (4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install indic_nlp_library==0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff41387e-5594-412a-b3d9-1742d9e1fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430d75b4-4f0e-4ac2-90c5-06d9f0221caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'nltk' from '/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/nltk/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d422ee-c685-48d0-97b6-b19612715cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfdeefd-2328-4310-adb1-d83fcb17a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0f4c6d-5d16-49fc-924c-2de9a90c1fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87235b0c-7403-4b03-8694-c791b79ad1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40 # maximum length of sentences\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "    \n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_sentences[index], self.target_sentences[index]\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, pad_value):\n",
    "    # Padding function to add pad_value to sequences until they reach max_len\n",
    "    for i in range(MAX_LENGTH - len(sequence)):\n",
    "        sequence.append(pad_value)\n",
    "    return sequence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1faa47dd-df60-4ef0-ac89-a6e62d154219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb451c6d-f7ab-4495-b88c-a4e46f158ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get english tokens...: 100%|██████████| 4093524/4093524 [00:20<00:00, 196204.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Hes', 'a', 'scientist', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get kannada tokens...: 100%|██████████| 4093524/4093524 [00:19<00:00, 212007.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ಇವರು', 'ಸಂಶೋಧಕ', 'ಸ್ವಭಾವದವರು', '.', '</s>']\n",
      "['linewrapping', 'gazelle', 'shetty', 'underside', 'Scepter', 'puzzled', 'wheeze', 'Lennon/Getty', 'DLRs', 'Rs4,150']\n",
      "['ಲೊಯೊಲಾ', 'ದೋಷಿಯಾದ', 'ನಾಚಿಕೆಪಡಲಿ', 'ಅಬಾಧಿತವಾದದ್ದು', 'ಮೇಲ್ಸೇತುವೆಗಳಿಗೆ', 'ಶಾಸಕನಾಗಬೇಕಿದೆ', 'underside', 'ಕಾವಲು', 'ಕಾಂಗ್ರೆಸ್ಸಿನದ್ದೇ', 'ಪರಿಗಣಿಸಬೇಕಾಗಿರುವ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get tokens from pre-processed files\n",
    "with open('Fine-Tuning-LLMs/translate/data/eng_tokens.txt', 'r') as f:\n",
    "    tokens = f.readlines()\n",
    "eng_tokens = []\n",
    "for x in trange(len(tokens), desc='get english tokens...'):\n",
    "    eng_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(eng_tokens[0])\n",
    "\n",
    "with open('Fine-Tuning-LLMs/translate/data/kan_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = f.readlines()\n",
    "kan_tokens = []\n",
    "for x in trange(len(tokens), desc='get kannada tokens...'):\n",
    "    kan_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(kan_tokens[0])\n",
    "\n",
    "# get vocabulary\n",
    "eng_vocab = set()\n",
    "kan_vocab = set()\n",
    "for i in eng_tokens:\n",
    "    for j in i:\n",
    "        eng_vocab.add(j)\n",
    "eng_vocab = list(eng_vocab)\n",
    "\n",
    "for i in kan_tokens:\n",
    "    for j in i:\n",
    "        kan_vocab.add(j)\n",
    "kan_vocab = list(kan_vocab)\n",
    "\n",
    "print(eng_vocab[:10])\n",
    "print(kan_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602f1d44-9265-4963-a6a1-af339ab2ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index lists\n",
    "eng_word2index = {word: index for index, word in enumerate(eng_vocab)}\n",
    "kan_word2index = {word: index for index, word in enumerate(kan_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d99ec3-d105-496e-adc8-19305c62d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644377\n"
     ]
    }
   ],
   "source": [
    "print(kan_word2index['</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86620327-2ce9-4ee5-9198-cf206c514825",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_indices = [[eng_word2index[word] for word in sent] for sent in eng_tokens] \n",
    "kan_indices = [[kan_word2index[word] for word in sent] for sent in kan_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba9196f-c1f5-4606-8694-624a2297ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219957, 205083, 243312, 258437, 37073, 233762]\n"
     ]
    }
   ],
   "source": [
    "print(eng_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4188b2db-36ab-4d06-862c-23971281a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate all sentences to length 40. or else the dataloader will not work\n",
    "eng_indices1 = [pad_sequence(sent[:MAX_LENGTH], eng_word2index['</s>']) for sent in eng_indices]\n",
    "kan_indices1 = [pad_sequence(sent[:MAX_LENGTH], kan_word2index['</s>']) for sent in kan_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2783b81-38d7-42c1-96b9-245e766e0416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([257024, 116113, 76049, 692843, 43440, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377, 644377], [219957, 205083, 243312, 258437, 37073, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762, 233762])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "\n",
    "dataset = TranslationDataset(kan_indices1, eng_indices1)\n",
    "print(dataset[0])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    #print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "864f8cfe-0880-4395-a4c5-18f6a7bb8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 5\n",
    "hidden_size = 128\n",
    "encoder = EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
    "optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd118810-7991-488f-b263-5342fa06f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training begin.\n",
      "batch took 9.14215087890625 sec\n",
      "batch took 6.058065414428711 sec\n",
      "batch took 5.827672243118286 sec\n",
      "batch took 5.854980230331421 sec\n",
      "batch took 5.841123104095459 sec\n",
      "batch took 5.847073793411255 sec\n",
      "batch took 5.853806495666504 sec\n",
      "batch took 5.850334882736206 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm.__iter__ at 0x2ad0ebac2510>\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/tqdm/std.py\", line 1179, in __iter__\n",
      "    yield obj\n",
      "KeyboardInterrupt: \n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-f7a64c240211>\", line 25, in <module>\n",
      "    loss.backward()\n",
      "  File \"/home/isaacson.d/.local/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/isaacson.d/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f7a64c240211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"training begin.\")\n",
    "import time\n",
    "\n",
    "# Training loop\n",
    "MODEL_SAVE_INTERVAL = 100 # save the model every so oftens\n",
    "losses = [] # average loss per epoch\n",
    "bar = trange(epochs, desc=f'')\n",
    "for epoch in bar:\n",
    "    epoch_loss = 0\n",
    "    for i, (kan_batch,eng_batch) in enumerate(dataloader): # TO-DO - Need to pad the data\n",
    "        time_start = time.time()\n",
    "        eng_batch = torch.stack(eng_batch, dim=1)\n",
    "        kan_batch = torch.stack(kan_batch, dim=1)\n",
    "\n",
    "        eng_batch = eng_batch.to(device)\n",
    "        kan_batch = kan_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
    "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
    "\n",
    "        loss = criterion(decoder_outputs.view(-1, len(eng_vocab)), eng_batch.view(-1))\n",
    "        epoch_loss += (loss.item()/len(eng_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % MODEL_SAVE_INTERVAL == 0:\n",
    "            torch.save(encoder.state_dict(), f\"encoder.pt\")\n",
    "            torch.save(decoder.state_dict(), f\"decoder.pt\")\n",
    "        \n",
    "        print(f\"batch took {time.time()-time_start} sec\")\n",
    "        \n",
    "    losses.append(epoch_loss)\n",
    "    bar.set_description(f'loss: {epoch_loss}')\n",
    "\n",
    "    if epoch % MODEL_SAVE_INTERVAL == 0:\n",
    "        torch.save(encoder.state_dict(), f\"encoder.pt\")\n",
    "        torch.save(decoder.state_dict(), f\"decoder.pt\")\n",
    "\n",
    "torch.save(encoder.state_dict(), f\"encoder_final.pt\")\n",
    "torch.save(decoder.state_dict(), f\"decoder_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7d351d-4c7d-490c-9f30-74a4d2e3921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_sentence):\n",
    "    # Preprocess input sentence\n",
    "    input_sequence = indic_tokenize.trivial_tokenize(input_sentence)\n",
    "    input_sequence.insert(0,\"<s>\")\n",
    "    input_sequence.append(\"</s>\")\n",
    "    vector =  [kan_word2index[word] for word in input_sequence]\n",
    "    input_sequence = pad_sequence(vector, kan_word2index['</s>'])\n",
    " \n",
    "    print(torch.tensor(input_sequence))\n",
    "    encoder_outputs, encoder_hidden = encoder(torch.tensor(input_sequence, device=device).view(-1, 1))\n",
    "    print(encoder_outputs)\n",
    "    decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
    " \n",
    "    _, topi = decoder_outputs.topk(1) # return largest output of the tensor\n",
    "    decoded_ids = topi.squeeze()\n",
    "    print(decoded_ids)\n",
    "\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids[0]:\n",
    "        if idx.item() == eng_word2index['</s>']:\n",
    "            decoded_words.append(eng_word2index['</s>'])\n",
    "            break\n",
    "        decoded_words.append(idx.item())\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d00c95c-448e-4497-aaa1-062a4119c8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indic_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-96331422dde6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print([key for key, val in eng_word2index.items() if val == idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meng_word2index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8fbff0eee527>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Preprocess input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindic_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrivial_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indic_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "out = translate_sentence('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು .')\n",
    "output = \"\"\n",
    "for idx in out:\n",
    "    #print([key for key, val in eng_word2index.items() if val == idx])\n",
    "    output +=  [key for key, val in eng_word2index.items() if val == idx][0] + \" \"\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084e1b8-b060-4dd4-80e9-a9d964947c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
