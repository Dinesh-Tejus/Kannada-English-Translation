{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGqCPnLC-12m",
        "outputId": "82b70c62-50ed-4094-cbf4-16784c7a33fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic_nlp_library==0.92\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-argparse (from indic_nlp_library==0.92)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic_nlp_library==0.92)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic_nlp_library==0.92)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic_nlp_library==0.92) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic_nlp_library==0.92) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic_nlp_library==0.92) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic_nlp_library==0.92) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic_nlp_library==0.92) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic_nlp_library==0.92) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic_nlp_library==0.92) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic_nlp_library==0.92)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic_nlp_library==0.92) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2024.2.2)\n",
            "Installing collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic_nlp_library\n",
            "Successfully installed indic_nlp_library-0.92 morfessor-2.0.6 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "!pip install indic_nlp_library==0.92\n",
        "from indicnlp.tokenize import indic_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 17 # maximum length of sentences\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return output, (hidden, cell)\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.lstm = nn.LSTM(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden[0].permute(1, 0, 2)  # For LSTM, hidden is a tuple (hidden_state, cell_state)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_lstm = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.lstm(input_lstm, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences):\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.source_sentences[index], self.target_sentences[index]\n",
        "\n",
        "\n",
        "def pad_sequence(sequence, pad_value):\n",
        "    # Padding function to add pad_value to sequences until they reach max_len\n",
        "    for i in range(MAX_LENGTH - len(sequence)):\n",
        "        sequence.append(pad_value)\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "x96u2BiX_Hep"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# get tokens from pre-processed files\n",
        "with open('eng_tokens_baby.txt', 'r') as f:\n",
        "    tokens = f.readlines()\n",
        "eng_tokens = []\n",
        "for x in trange(len(tokens), desc='get english tokens...'):\n",
        "    eng_tokens.append(tokens[x].strip('\\n').split(' '))\n",
        "print(eng_tokens[0])\n",
        "\n",
        "with open('kan_tokens_baby.txt', 'r', encoding='utf-8') as f:\n",
        "    tokens = f.readlines()\n",
        "kan_tokens = []\n",
        "for x in trange(len(tokens), desc='get kannada tokens...'):\n",
        "    kan_tokens.append(tokens[x].strip('\\n').split(' '))\n",
        "print(kan_tokens[0])\n",
        "\n",
        "# get vocabulary\n",
        "eng_vocab = set()\n",
        "kan_vocab = set()\n",
        "for i in eng_tokens:\n",
        "    for j in i:\n",
        "        eng_vocab.add(j)\n",
        "eng_vocab = list(eng_vocab)\n",
        "\n",
        "for i in kan_tokens:\n",
        "    for j in i:\n",
        "        kan_vocab.add(j)\n",
        "kan_vocab = list(kan_vocab)\n",
        "\n",
        "print(eng_vocab[:10])\n",
        "print(kan_vocab[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIdm8SXA_Qez",
        "outputId": "ffa02f85-0a36-46b3-a6a8-ad39aae9ace0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get english tokens...: 100%|██████████| 10000/10000 [00:00<00:00, 451612.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'Hes', 'a', 'scientist', '.', '</s>']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get kannada tokens...: 100%|██████████| 10000/10000 [00:00<00:00, 227190.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'ಇವರು', 'ಸಂಶೋಧಕ', 'ಸ್ವಭಾವದವರು', '.', '</s>']\n",
            "['gracious', 'earliest', 'cope', 'Sirisena', 'Diabetes', 'pans', 'considering', 'guts', 'produces', 'uncertainties']\n",
            "['ಏಕದಿನ', 'ಚಿಂತೆಗಳು', 'ರೀತಿಯ', 'ಮಿಡಿಯಿತು………', 'ಆಟದ', 'ಇದು”', 'ಬಯಸುವವರು', 'ಏಕತೆಯೇ', 'ಪೂರ್ವಸಿದ್ಧತೆಗಳನ್ನು', 'ಅಂತಿಮಗೊಂಡಿಲ್ಲ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get index lists\n",
        "eng_word2index = {word: index for index, word in enumerate(eng_vocab)}\n",
        "kan_word2index = {word: index for index, word in enumerate(kan_vocab)}"
      ],
      "metadata": {
        "id": "ZMuFSLJ-_WBq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_indices = [[eng_word2index[word] for word in sent] for sent in eng_tokens]\n",
        "kan_indices = [[kan_word2index[word] for word in sent] for sent in kan_tokens]"
      ],
      "metadata": {
        "id": "IPXDGcqS_WUX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_indices_padded = []\n",
        "kan_indices_padded = []\n",
        "\n",
        "for eng_sent, kan_sent in zip(eng_tokens, kan_tokens):\n",
        "    if len(eng_sent) <= MAX_LENGTH and len(kan_sent) <= MAX_LENGTH:\n",
        "        eng_indices_padded.append(pad_sequence([eng_word2index[word] for word in eng_sent], eng_word2index['</s>']))\n",
        "        kan_indices_padded.append(pad_sequence([kan_word2index[word] for word in kan_sent], kan_word2index['</s>']))\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "\n",
        "# Ensure that both lists have the same length\n",
        ""
      ],
      "metadata": {
        "id": "IawnwD44_Yxj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(len(eng_indices_padded) == len(kan_indices_padded), f\"Number of English sentences: {len(eng_indices_padded)}, Number of Kannada sentences: {len(kan_indices_padded)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgDDzUHT_aE9",
        "outputId": "b67f9716-810b-476b-a65c-963e2ee6c18e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Number of English sentences: 7957, Number of Kannada sentences: 7957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kan_train, kan_test, eng_train, eng_test = train_test_split(kan_indices_padded, eng_indices_padded, test_size=0.3)\n",
        "\n",
        "# Create datasets and dataloaders for train and test sets\n",
        "train_dataset = TranslationDataset(kan_train, eng_train)\n",
        "test_dataset = TranslationDataset(kan_test, eng_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bbcaqT25_cCY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 24\n",
        "eng_indices1 = eng_indices_padded[:10000]\n",
        "kan_indices1 = kan_indices_padded[:10000]\n",
        "dataset = TranslationDataset(kan_indices1, eng_indices1)\n",
        "print(dataset[0])\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5-6Zm-S_eu5",
        "outputId": "72e6896d-75a7-4c14-e9e3-adf6f049a21f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([8521, 24184, 24768, 13704, 22693, 19719, 19719, 19719, 19719, 19719, 19719, 19719, 19719, 19719, 19719, 19719, 19719], [3584, 13263, 14098, 9398, 4549, 13123, 13123, 13123, 13123, 13123, 13123, 13123, 13123, 13123, 13123, 13123, 13123])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "epochs = 500\n",
        "hidden_size = 128\n",
        "encoder = EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
        "optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "GbPaISFM_iOc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if os.path.exists(\"encoder1.pth\") and os.path.exists(\"decoder1.pth\"):\n",
        "    encoder =  EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)  # Replace YourEncoderModelClass with the actual class of your encoder model\n",
        "    decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
        "\n",
        "    encoder.load_state_dict(torch.load(\"encoder_final1.pth\"))\n",
        "    decoder.load_state_dict(torch.load(\"decoder_final1.pth\"))\n",
        "    print(\"pre-trained models loaded.\")\n",
        "\n",
        "\n",
        "print(\"training begin.\")\n",
        "import time\n",
        "\n",
        "# Training loop\n",
        "MODEL_SAVE_INTERVAL = 10 # save the model every so oftens\n",
        "losses = [] # average loss per epoch\n",
        "bar = trange(epochs, desc=f'')\n",
        "for epoch in bar:\n",
        "    epoch_loss = 0\n",
        "    for i, (kan_batch,eng_batch) in enumerate(dataloader): # TO-DO - Need to pad the data\n",
        "        time_start = time.time()\n",
        "        eng_batch = torch.stack(eng_batch, dim=1)\n",
        "        kan_batch = torch.stack(kan_batch, dim=1)\n",
        "\n",
        "        eng_batch = eng_batch.to(device)\n",
        "        kan_batch = kan_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
        "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
        "\n",
        "        loss = criterion(decoder_outputs.view(-1, len(eng_vocab)), eng_batch.view(-1))\n",
        "        epoch_loss += (loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % MODEL_SAVE_INTERVAL == 0:\n",
        "            torch.save(encoder.state_dict(), f\"encoder1.pth\")\n",
        "            torch.save(decoder.state_dict(), f\"decoder1.pth\")\n",
        "\n",
        "#         print(f\"batch took {time.time()-time_start} sec\")\n",
        "    epoch_loss /= len(eng_batch)\n",
        "    losses.append(epoch_loss)\n",
        "    bar.set_description(f'loss: {epoch_loss}')\n",
        "\n",
        "    if epoch % MODEL_SAVE_INTERVAL == 0:\n",
        "        torch.save(encoder.state_dict(), f\"encoder1.pth\")\n",
        "        torch.save(decoder.state_dict(), f\"decoder1.pth\")\n",
        "\n",
        "torch.save(encoder.state_dict(), f\"encoder_final1.pth\")\n",
        "torch.save(decoder.state_dict(), f\"decoder_final1.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qheAvPKc_nZA",
        "outputId": "cf9e48db-6e2a-49d2-aadb-0c528f29f1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre-trained models loaded.\n",
            "training begin.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 9.202878100367693:   8%|▊         | 41/500 [10:15<1:54:55, 15.02s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"encoder1.pth\") and os.path.exists(\"decoder1.pth\"):\n",
        "    encoder =  EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)  # Replace YourEncoderModelClass with the actual class of your encoder model\n",
        "    decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
        "\n",
        "    encoder.load_state_dict(torch.load(\"encoder_final1.pth\"))\n",
        "    decoder.load_state_dict(torch.load(\"decoder_final1.pth\"))\n",
        "    print(\"pre-trained models loaded.\")\n"
      ],
      "metadata": {
        "id": "CwA3msU3_ooO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(input_sentence):\n",
        "    # Preprocess input sentence\n",
        "    input_sequence = indic_tokenize.trivial_tokenize(input_sentence)\n",
        "    input_sequence.insert(0,\"<s>\")\n",
        "    input_sequence.append(\"</s>\")\n",
        "    vector =  [kan_word2index[word] for word in input_sequence]\n",
        "    input_sequence = pad_sequence(vector, kan_word2index['</s>'])\n",
        "\n",
        "    print(torch.tensor(input_sequence))\n",
        "    encoder_outputs, encoder_hidden = encoder(torch.tensor(input_sequence, device=device).view(-1, 1))\n",
        "#     print(encoder_outputs)\n",
        "    decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "    _, topi = decoder_outputs.topk(1) # return largest output of the tensor\n",
        "    decoded_ids = topi.squeeze()\n",
        "#     print(decoded_ids)\n",
        "\n",
        "    decoded_words = []\n",
        "    for idx in decoded_ids[0]:\n",
        "        if idx.item() == eng_word2index['</s>']:\n",
        "            decoded_words.append(eng_word2index['</s>'])\n",
        "            break\n",
        "        decoded_words.append(idx.item())\n",
        "\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "o8f6c2m-_s2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = translate_sentence( 'ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು')\n",
        "output = \"\"\n",
        "for idx in out:\n",
        "    #print([key for key, val in eng_word2index.items() if val == idx])\n",
        "    output +=  [key for key, val in eng_word2index.items() if val == idx][0] + \" \"\n",
        "output"
      ],
      "metadata": {
        "id": "qbkoQNv5_yqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, dataloader, reference_translations):\n",
        "    references = []\n",
        "    translations = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for kan_batch, eng_batch in tqdm(dataloader, desc='Evaluating'):\n",
        "            kan_batch = torch.stack(kan_batch, dim=1).to(device)\n",
        "            eng_batch = torch.stack(eng_batch, dim=1).to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
        "\n",
        "            _, preds = decoder_outputs.max(2)\n",
        "\n",
        "            references.extend(eng_batch.cpu().numpy().tolist())\n",
        "            translations.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "            # Calculate accuracy and precision\n",
        "            for pred_sent, ref_sent in zip(preds.cpu().numpy().tolist(), eng_batch.cpu().numpy().tolist()):\n",
        "                total_predictions += len(ref_sent)\n",
        "                for pred_token, ref_token in zip(pred_sent, ref_sent):\n",
        "                    if pred_token == ref_token:\n",
        "                        correct_predictions += 1\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    # bleu_score = corpus_bleu(reference_translations, translations)\n",
        "\n",
        "    # Calculate accuracy and precision\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    precision = accuracy  # Assuming precision and accuracy are the same in this context\n",
        "\n",
        "    return 0, accuracy, precision\n",
        "\n",
        "# Evaluate the model\n",
        "bleu_score, accuracy, precision = evaluate(encoder, decoder, test_dataloader, eng_test)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n"
      ],
      "metadata": {
        "id": "GbOgWB0X_y8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(encoder, decoder, dataloader, reference_translations):\n",
        "    references = []\n",
        "    translations = []\n",
        "    with torch.no_grad():\n",
        "        for kan_batch, eng_batch in tqdm(dataloader, desc='Evaluating'):\n",
        "            kan_batch = torch.stack(kan_batch, dim=1).to(device)\n",
        "            eng_batch = torch.stack(eng_batch, dim=1).to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
        "\n",
        "            _, preds = decoder_outputs.max(2)\n",
        "\n",
        "            for pred_sent, ref_sent in zip(preds.cpu().numpy().tolist(), eng_batch.cpu().numpy().tolist()):\n",
        "                # Convert indices back to tokens\n",
        "                pred_tokens = [eng_vocab[token_idx] for token_idx in pred_sent if token_idx != eng_word2index['</s>']]\n",
        "                ref_tokens = [eng_vocab[token_idx] for token_idx in ref_sent if token_idx != eng_word2index['</s>']]\n",
        "                references.append([ref_tokens])\n",
        "                translations.append(pred_tokens)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = corpus_bleu(references, translations)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Evaluate the model\n",
        "bleu_score = evaluate(encoder, decoder, test_dataloader, eng_test)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ],
      "metadata": {
        "id": "QAiVQA80_0EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KyZBQ6Nj_1cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C842jboNGR4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}