{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d962c41-9c1b-4d98-8f03-36ca6a912d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.0.1 in ./.local/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: networkx in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (2.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: typing-extensions in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (3.7.4.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: filelock in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch==2.0.1) (1.8)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.8/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: wheel in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.36.2)\n",
      "Requirement already satisfied: setuptools in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1) (18.1.3)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1) (3.29.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch==2.0.1) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch==2.0.1) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch==2.0.1) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b46f81-36f3-48fd-b5c2-c50fd1f2309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: indic_nlp_library==0.92 in ./.local/lib/python3.8/site-packages (0.92)\n",
      "Requirement already satisfied: pandas in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from indic_nlp_library==0.92) (1.2.4)\n",
      "Requirement already satisfied: morfessor in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (2.0.6)\n",
      "Requirement already satisfied: sphinx-rtd-theme in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (2.0.0)\n",
      "Requirement already satisfied: sphinx-argparse in ./.local/lib/python3.8/site-packages (from indic_nlp_library==0.92) (0.4.0)\n",
      "Requirement already satisfied: numpy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from indic_nlp_library==0.92) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from pandas->indic_nlp_library==0.92) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from pandas->indic_nlp_library==0.92) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->indic_nlp_library==0.92) (1.15.0)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in ./.local/lib/python3.8/site-packages (from sphinx-argparse->indic_nlp_library==0.92) (5.1.1)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.1)\n",
      "Requirement already satisfied: Pygments>=2.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.20,>=0.14 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (0.17.1)\n",
      "Requirement already satisfied: packaging in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (20.9)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.11.3)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.1.0)\n",
      "Requirement already satisfied: imagesize in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.9.0)\n",
      "Requirement already satisfied: requests>=2.5.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.25.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.3)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.0.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (0.7.12)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.local/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (7.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from importlib-metadata>=4.4->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library==0.92) (2.4.7)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in ./.local/lib/python3.8/site-packages (from sphinx-rtd-theme->indic_nlp_library==0.92) (4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install indic_nlp_library==0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff41387e-5594-412a-b3d9-1742d9e1fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "430d75b4-4f0e-4ac2-90c5-06d9f0221caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'nltk' from '/shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages/nltk/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d422ee-c685-48d0-97b6-b19612715cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdfdeefd-2328-4310-adb1-d83fcb17a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee0f4c6d-5d16-49fc-924c-2de9a90c1fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   5360 MiB |   6725 MiB | 513622 GiB | 513617 GiB |\\n|       from large pool |   5354 MiB |   6718 MiB | 510168 GiB | 510163 GiB |\\n|       from small pool |      6 MiB |     21 MiB |   3453 GiB |   3453 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   5360 MiB |   6725 MiB | 513622 GiB | 513617 GiB |\\n|       from large pool |   5354 MiB |   6718 MiB | 510168 GiB | 510163 GiB |\\n|       from small pool |      6 MiB |     21 MiB |   3453 GiB |   3453 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   5357 MiB |   6721 MiB | 513388 GiB | 513383 GiB |\\n|       from large pool |   5351 MiB |   6714 MiB | 509937 GiB | 509932 GiB |\\n|       from small pool |      5 MiB |     21 MiB |   3451 GiB |   3451 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   6202 MiB |   8150 MiB |   9730 MiB |   3528 MiB |\\n|       from large pool |   6186 MiB |   8128 MiB |   9704 MiB |   3518 MiB |\\n|       from small pool |     16 MiB |     22 MiB |     26 MiB |     10 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |    841 MiB |   2417 MiB | 394286 GiB | 394286 GiB |\\n|       from large pool |    831 MiB |   2416 MiB | 390826 GiB | 390825 GiB |\\n|       from small pool |      9 MiB |     15 MiB |   3460 GiB |   3460 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      85    |     341    |   36263 K  |   36263 K  |\\n|       from large pool |      23    |      41    |    6014 K  |    6014 K  |\\n|       from small pool |      62    |     310    |   30248 K  |   30248 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      85    |     341    |   36263 K  |   36263 K  |\\n|       from large pool |      23    |      41    |    6014 K  |    6014 K  |\\n|       from small pool |      62    |     310    |   30248 K  |   30248 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      20    |      26    |      32    |      12    |\\n|       from large pool |      12    |      15    |      19    |       7    |\\n|       from small pool |       8    |      11    |      13    |       5    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      37    |      44    |   15368 K  |   15368 K  |\\n|       from large pool |       9    |      12    |    2119 K  |    2119 K  |\\n|       from small pool |      28    |      36    |   13248 K  |   13248 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87235b0c-7403-4b03-8694-c791b79ad1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 17 # maximum length of sentences\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "    \n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_sentences[index], self.target_sentences[index]\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, pad_value):\n",
    "    # Padding function to add pad_value to sequences until they reach max_len\n",
    "    for i in range(MAX_LENGTH - len(sequence)):\n",
    "        sequence.append(pad_value)\n",
    "    return sequence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1faa47dd-df60-4ef0-ac89-a6e62d154219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb451c6d-f7ab-4495-b88c-a4e46f158ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get english tokens...: 100%|██████████| 4093524/4093524 [00:21<00:00, 191258.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Hes', 'a', 'scientist', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get kannada tokens...: 100%|██████████| 4093524/4093524 [00:06<00:00, 639107.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ಇವರು', 'ಸಂಶೋಧಕ', 'ಸ್ವಭಾವದವರು', '.', '</s>']\n",
      "['Fakira', 'Newwz', 'Lank', 'Soldevanahalli', 'dualSIM', 'Yafan', 'Rushikonda', 'Chittade', 'dewed', 'motif']\n",
      "['ಒಬ್ಬಂಟಿಯಾಗಿರಲಿಲ್ಲ’', 'ಡ್ರೈವಿನಲ್ಲಿನ', 'ವಿವಿಧೋದ್ದೇಶ', 'ಡ್ಯಾಮಿಯೆನ್', 'ಕನೆಕ್ಟಿವಿಟಿಗಳನ್ನು', 'ನಾಯ್ಕೋಡಿ', '2018’\\xa0', 'ಸಾರಿಕೊಳ್ಳುತ್ತಿದ್ದರು', 'ಲಗತ್ತಿಸುವುದು', 'ಆತ್ಮಗೌರವವುಳ್ಳವರಾಗಿದ್ದಾರೆ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get tokens from pre-processed files\n",
    "with open('eng_tokens.txt', 'r') as f:\n",
    "    tokens = f.readlines()\n",
    "eng_tokens = []\n",
    "for x in trange(len(tokens), desc='get english tokens...'):\n",
    "    eng_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(eng_tokens[0])\n",
    "\n",
    "with open('kan_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = f.readlines()\n",
    "kan_tokens = []\n",
    "for x in trange(len(tokens), desc='get kannada tokens...'):\n",
    "    kan_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(kan_tokens[0])\n",
    "\n",
    "# get vocabulary\n",
    "eng_vocab = set()\n",
    "kan_vocab = set()\n",
    "for i in eng_tokens:\n",
    "    for j in i:\n",
    "        eng_vocab.add(j)\n",
    "eng_vocab = list(eng_vocab)\n",
    "\n",
    "for i in kan_tokens:\n",
    "    for j in i:\n",
    "        kan_vocab.add(j)\n",
    "kan_vocab = list(kan_vocab)\n",
    "\n",
    "print(eng_vocab[:10])\n",
    "print(kan_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "602f1d44-9265-4963-a6a1-af339ab2ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index lists\n",
    "eng_word2index = {word: index for index, word in enumerate(eng_vocab)}\n",
    "kan_word2index = {word: index for index, word in enumerate(kan_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18d99ec3-d105-496e-adc8-19305c62d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265648\n"
     ]
    }
   ],
   "source": [
    "print(kan_word2index['</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86620327-2ce9-4ee5-9198-cf206c514825",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_indices = [[eng_word2index[word] for word in sent] for sent in eng_tokens] \n",
    "kan_indices = [[kan_word2index[word] for word in sent] for sent in kan_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bba9196f-c1f5-4606-8694-624a2297ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4093524\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4188b2db-36ab-4d06-862c-23971281a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate all sentences to length 40. or else the dataloader will not work\n",
    "# eng_indices_padded = [pad_sequence(sent, eng_word2index['</s>']) if len(sent) <= MAX_LENGHT for sent in eng_indices]\n",
    "eng_indices_padded = [pad_sequence(sent, eng_word2index['</s>']) if len(sent) <= MAX_LENGTH else None for sent in eng_indices]\n",
    "eng_indices_padded = [sent for sent in eng_indices_padded if sent is not None]\n",
    "kan_indices_padded = [pad_sequence(sent, kan_word2index['</s>']) if len(sent) <= MAX_LENGTH else None for sent in kan_indices]\n",
    "kan_indices_padded = [sent for sent in kan_indices_padded if sent is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd8b98df-4bfe-4763-ae5a-b60f334c890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_indices_padded = []\n",
    "kan_indices_padded = []\n",
    "\n",
    "for eng_sent, kan_sent in zip(eng_tokens, kan_tokens):\n",
    "    if len(eng_sent) <= MAX_LENGTH and len(kan_sent) <= MAX_LENGTH:\n",
    "        eng_indices_padded.append(pad_sequence([eng_word2index[word] for word in eng_sent], eng_word2index['</s>']))\n",
    "        kan_indices_padded.append(pad_sequence([kan_word2index[word] for word in kan_sent], kan_word2index['</s>']))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Ensure that both lists have the same length\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4f16f9a-77f3-4fe4-a13b-76558c3cb976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Number of English sentences: 3260760, Number of Kannada sentences: 3260760\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(eng_indices_padded) == len(kan_indices_padded), f\"Number of English sentences: {len(eng_indices_padded)}, Number of Kannada sentences: {len(kan_indices_padded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "168921b3-c478-4470-b28f-5af9c5fdfa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3260760\n",
      "[2241, 239009, 91612, 199834, 314685, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848]\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_indices_padded))\n",
    "print(eng_indices_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f23e75db-9dc3-4bf5-8480-e0a8537e00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_indices1 = eng_indices_padded[:5000]\n",
    "kan_indices1 = kan_indices_padded[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2783b81-38d7-42c1-96b9-245e766e0416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([743458, 1008859, 345974, 565182, 1478997, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648, 1265648], [2241, 239009, 91612, 199834, 314685, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848, 131848])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "\n",
    "dataset = TranslationDataset(kan_indices1, eng_indices1)\n",
    "print(dataset[0])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd14d392-ba95-4854-bd12-a4d7a40df993",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 200\n",
    "hidden_size = 128\n",
    "encoder = EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
    "optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd118810-7991-488f-b263-5342fa06f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained models loaded.\n",
      "training begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 6.134352413316565:   0%|          | 1/200 [01:48<6:01:12, 108.91s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if os.path.exists(\"encoder_final.pt\") and os.path.exists(\"decoder_final.pt\"):\n",
    "    encoder =  EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)  # Replace YourEncoderModelClass with the actual class of your encoder model\n",
    "    decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(\"encoder_final.pt\"))\n",
    "    decoder.load_state_dict(torch.load(\"decoder_final.pt\"))\n",
    "    print(\"pre-trained models loaded.\")\n",
    "\n",
    "   \n",
    "    \n",
    "print(\"training begin.\")\n",
    "import time\n",
    "\n",
    "# Training loop\n",
    "MODEL_SAVE_INTERVAL = 10 # save the model every so oftens\n",
    "losses = [] # average loss per epoch\n",
    "bar = trange(epochs, desc=f'')\n",
    "for epoch in bar:\n",
    "    epoch_loss = 0\n",
    "    for i, (kan_batch,eng_batch) in enumerate(dataloader): # TO-DO - Need to pad the data\n",
    "        time_start = time.time()\n",
    "        eng_batch = torch.stack(eng_batch, dim=1)\n",
    "        kan_batch = torch.stack(kan_batch, dim=1)\n",
    "\n",
    "        eng_batch = eng_batch.to(device)\n",
    "        kan_batch = kan_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
    "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
    "\n",
    "        loss = criterion(decoder_outputs.view(-1, len(eng_vocab)), eng_batch.view(-1))\n",
    "        epoch_loss += (loss.item()/len(eng_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % MODEL_SAVE_INTERVAL == 0:\n",
    "            torch.save(encoder.state_dict(), f\"encoder.pt\")\n",
    "            torch.save(decoder.state_dict(), f\"decoder.pt\")\n",
    "        \n",
    "#         print(f\"batch took {time.time()-time_start} sec\")\n",
    "        \n",
    "    losses.append(epoch_loss)\n",
    "    bar.set_description(f'loss: {epoch_loss}')\n",
    "\n",
    "    if epoch % MODEL_SAVE_INTERVAL == 0:\n",
    "        torch.save(encoder.state_dict(), f\"encoder.pt\")\n",
    "        torch.save(decoder.state_dict(), f\"decoder.pt\")\n",
    "\n",
    "torch.save(encoder.state_dict(), f\"encoder_final.pt\")\n",
    "torch.save(decoder.state_dict(), f\"decoder_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa3430f2-b89d-4193-af7b-9de923c2a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained models loaded.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"encoder_final.pt\") and os.path.exists(\"decoder_final.pt\"):\n",
    "    encoder =  EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)  # Replace YourEncoderModelClass with the actual class of your encoder model\n",
    "    decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(\"encoder_final.pt\"))\n",
    "    decoder.load_state_dict(torch.load(\"decoder_final.pt\"))\n",
    "    print(\"pre-trained models loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d351d-4c7d-490c-9f30-74a4d2e3921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_sentence):\n",
    "    # Preprocess input sentence\n",
    "    input_sequence = indic_tokenize.trivial_tokenize(input_sentence)\n",
    "    input_sequence.insert(0,\"<s>\")\n",
    "    input_sequence.append(\"</s>\")\n",
    "    vector =  [kan_word2index[word] for word in input_sequence]\n",
    "    input_sequence = pad_sequence(vector, kan_word2index['</s>'])\n",
    " \n",
    "    print(torch.tensor(input_sequence))\n",
    "    encoder_outputs, encoder_hidden = encoder(torch.tensor(input_sequence, device=device).view(-1, 1))\n",
    "    print(encoder_outputs)\n",
    "    decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
    " \n",
    "    _, topi = decoder_outputs.topk(1) # return largest output of the tensor\n",
    "    decoded_ids = topi.squeeze()\n",
    "    print(decoded_ids)\n",
    "\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids[0]:\n",
    "        if idx.item() == eng_word2index['</s>']:\n",
    "            decoded_words.append(eng_word2index['</s>'])\n",
    "            break\n",
    "        decoded_words.append(idx.item())\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00c95c-448e-4497-aaa1-062a4119c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_sentence('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು .')\n",
    "output = \"\"\n",
    "for idx in out:\n",
    "    #print([key for key, val in eng_word2index.items() if val == idx])\n",
    "    output +=  [key for key, val in eng_word2index.items() if val == idx][0] + \" \"\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084e1b8-b060-4dd4-80e9-a9d964947c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
