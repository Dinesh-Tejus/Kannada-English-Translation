{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40 # maximum length of sentences\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "    \n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.source_sentences[index], self.target_sentences[index]\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, pad_value):\n",
    "  # Padding function to add pad_value to sequences until they reach max_len\n",
    "  for i in range(MAX_LENGTH - len(sequence)):\n",
    "      sequence.append(pad_value)\n",
    "  return sequence  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get english tokens...: 100%|██████████| 4093524/4093524 [00:44<00:00, 91227.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Hes', 'a', 'scientist', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get kannada tokens...: 100%|██████████| 4093524/4093524 [04:59<00:00, 13662.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ಇವರು', 'ಸಂಶೋಧಕ', 'ಸ್ವಭಾವದವರು', '.', '</s>']\n",
      "['ACHARYA', 'Roulette', 'Bamleshwari', 'exclusivist', 'staging', 'Fairtrade', 'Teat', 'Jugaad', 'Ananantnag', '32.352']\n",
      "['ಶೇಖರಿಸಿ', 'ಕಾರ್ಟಿಂಗ್', 'ಎಬಿಸಿಡಿಯನ್ನು', 'ನವಲ್\\u200b', 'ಹೇಳಿದ್ದರೇ', 'ಬಿಟ್ಟುಕೊಡಿರಿ', 'ದೇಶದ್ರೋಹಕ್ಕೆ', 'ನನಗೆನೂ', 'ಡಿಬಗ್', 'ಕೇಂದ್ರದಿAದ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get tokens from pre-processed files\n",
    "with open('data/eng_tokens.txt', 'r') as f:\n",
    "    tokens = f.readlines()\n",
    "eng_tokens = []\n",
    "for x in trange(len(tokens), desc='get english tokens...'):\n",
    "    eng_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(eng_tokens[0])\n",
    "\n",
    "with open('data/kan_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = f.readlines()\n",
    "kan_tokens = []\n",
    "for x in trange(len(tokens), desc='get kannada tokens...'):\n",
    "    kan_tokens.append(tokens[x].strip('\\n').split(' '))\n",
    "print(kan_tokens[0])\n",
    "\n",
    "# get vocabulary\n",
    "eng_vocab = set()\n",
    "kan_vocab = set()\n",
    "for i in eng_tokens:\n",
    "  for j in i:\n",
    "    eng_vocab.add(j)\n",
    "eng_vocab = list(eng_vocab)\n",
    "\n",
    "for i in kan_tokens:\n",
    "  for j in i:\n",
    "    kan_vocab.add(j)\n",
    "kan_vocab = list(kan_vocab)\n",
    "\n",
    "print(eng_vocab[:10])\n",
    "print(kan_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index lists\n",
    "eng_word2index = {word: index for index, word in enumerate(eng_vocab)}\n",
    "kan_word2index = {word: index for index, word in enumerate(kan_vocab)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915327\n"
     ]
    }
   ],
   "source": [
    "print(kan_word2index['</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_indices = [[eng_word2index[word] for word in sent] for sent in eng_tokens] \n",
    "kan_indices = [[kan_word2index[word] for word in sent] for sent in kan_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39465, 5492, 162182, 227654, 142844, 149941]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(eng_indices[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43meng_word2index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(eng_indices[0])\n",
    "print(eng_word2index.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39465, 284404, 181139, 5473, 72987, 75228, 111596, 3203, 217934, 283968, 170018, 148156, 43673, 119737, 74278, 105664, 283968, 81580, 256019, 178387, 176908, 197455, 14698, 161885, 85055, 149941]\n"
     ]
    }
   ],
   "source": [
    "print(eng_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_indices1 = [pad_sequence(sent, eng_word2index['</s>']) for sent in eng_indices[:10]]\n",
    "kan_indices1 = [pad_sequence(sent, kan_word2index['</s>']) for sent in kan_indices[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan [[786989, 457540, 492223, 207483, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 315690, 990778, 1421846, 350639, 223991, 179123, 1371837, 173821, 1082666, 428419, 95888, 315690, 315690, 977259, 1249782, 970673, 697608, 677901, 1238968, 315690, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 342898, 87511, 567903, 1049205, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 68855, 754679, 630051, 1265168, 222379, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 247647, 1067324, 1160534, 64319, 1120361, 157890, 827993, 1197435, 1192319, 311388, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 1162788, 1435340, 905644, 1331877, 948668, 408439, 805902, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 38929, 1195313, 1111803, 736812, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 323127, 1350787, 805902, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 26588, 100830, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327], [786989, 1045684, 997823, 1462632, 716675, 307981, 888138, 357443, 611688, 537984, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327]]\n",
      "eng [[39465, 5492, 162182, 227654, 142844, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 284404, 181139, 5473, 72987, 75228, 111596, 3203, 217934, 283968, 170018, 148156, 43673, 119737, 74278, 105664, 283968, 81580, 256019, 178387, 176908, 197455, 14698, 161885, 85055, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 74303, 156704, 315774, 229672, 64770, 306255, 142844, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 64633, 270694, 162182, 50740, 264922, 134890, 105502, 15491, 142844, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 157160, 215393, 75809, 56964, 1623, 72987, 300119, 281279, 48042, 315606, 184571, 124454, 162743, 56943, 72987, 39473, 198704, 142844, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 250766, 108344, 296757, 75761, 140363, 103555, 196650, 48398, 55867, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 177974, 85413, 64633, 89393, 162922, 203803, 142844, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 241753, 186590, 116041, 55867, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 197318, 148250, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941], [39465, 177997, 177997, 256019, 73554, 123098, 276648, 72987, 217041, 300792, 55750, 98143, 196488, 142844, 256019, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941]]\n",
      "[[tensor([786989, 786989, 786989, 786989, 786989, 786989, 786989, 786989, 786989,\n",
      "        786989]), tensor([ 457540, 1162788, 1045684,  247647,   68855,   38929,  342898,   26588,\n",
      "         315690,  323127]), tensor([ 492223, 1435340,  997823, 1067324,  754679, 1195313,   87511,  100830,\n",
      "         990778, 1350787]), tensor([ 207483,  905644, 1462632, 1160534,  630051, 1111803,  567903,  915327,\n",
      "        1421846,  805902]), tensor([ 537984, 1331877,  716675,   64319, 1265168,  736812, 1049205,  915327,\n",
      "         350639,  915327]), tensor([ 915327,  948668,  307981, 1120361,  222379,  537984,  537984,  915327,\n",
      "         223991,  915327]), tensor([915327, 408439, 888138, 157890, 537984, 915327, 915327, 915327, 179123,\n",
      "        915327]), tensor([ 915327,  805902,  357443,  827993,  915327,  915327,  915327,  915327,\n",
      "        1371837,  915327]), tensor([ 915327,  915327,  611688, 1197435,  915327,  915327,  915327,  915327,\n",
      "         173821,  915327]), tensor([ 915327,  915327,  537984, 1192319,  915327,  915327,  915327,  915327,\n",
      "        1082666,  915327]), tensor([915327, 915327, 915327, 311388, 915327, 915327, 915327, 915327, 428419,\n",
      "        915327]), tensor([915327, 915327, 915327, 537984, 915327, 915327, 915327, 915327,  95888,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 315690,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 315690,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 977259,\n",
      "        915327]), tensor([ 915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "        1249782,  915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 970673,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 697608,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 677901,\n",
      "        915327]), tensor([ 915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "        1238968,  915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 315690,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327]), tensor([915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327])], [tensor([39465, 39465, 39465, 39465, 39465, 39465, 39465, 39465, 39465, 39465]), tensor([  5492, 250766, 177997, 157160,  64633, 177974,  74303, 197318, 284404,\n",
      "        241753]), tensor([162182, 108344, 177997, 215393, 270694,  85413, 156704, 148250, 181139,\n",
      "        186590]), tensor([227654, 296757, 256019,  75809, 162182,  64633, 315774, 149941,   5473,\n",
      "        116041]), tensor([142844,  75761,  73554,  56964,  50740,  89393, 229672, 149941,  72987,\n",
      "         55867]), tensor([149941, 140363, 123098,   1623, 264922, 162922,  64770, 149941,  75228,\n",
      "        149941]), tensor([149941, 103555, 276648,  72987, 134890, 203803, 306255, 149941, 111596,\n",
      "        149941]), tensor([149941, 196650,  72987, 300119, 105502, 142844, 142844, 149941,   3203,\n",
      "        149941]), tensor([149941,  48398, 217041, 281279,  15491, 149941, 149941, 149941, 217934,\n",
      "        149941]), tensor([149941,  55867, 300792,  48042, 142844, 149941, 149941, 149941, 283968,\n",
      "        149941]), tensor([149941, 149941,  55750, 315606, 149941, 149941, 149941, 149941, 170018,\n",
      "        149941]), tensor([149941, 149941,  98143, 184571, 149941, 149941, 149941, 149941, 148156,\n",
      "        149941]), tensor([149941, 149941, 196488, 124454, 149941, 149941, 149941, 149941,  43673,\n",
      "        149941]), tensor([149941, 149941, 142844, 162743, 149941, 149941, 149941, 149941, 119737,\n",
      "        149941]), tensor([149941, 149941, 256019,  56943, 149941, 149941, 149941, 149941,  74278,\n",
      "        149941]), tensor([149941, 149941, 149941,  72987, 149941, 149941, 149941, 149941, 105664,\n",
      "        149941]), tensor([149941, 149941, 149941,  39473, 149941, 149941, 149941, 149941, 283968,\n",
      "        149941]), tensor([149941, 149941, 149941, 198704, 149941, 149941, 149941, 149941,  81580,\n",
      "        149941]), tensor([149941, 149941, 149941, 142844, 149941, 149941, 149941, 149941, 256019,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 178387,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 176908,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 197455,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,  14698,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 161885,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,  85055,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941]), tensor([149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941, 149941,\n",
      "        149941])]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "print(\"kan\",kan_indices1[:10])\n",
    "print(\"eng\",eng_indices1[:10])\n",
    "\n",
    "dataset = TranslationDataset(kan_indices1[:10], eng_indices1[:10])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 5\n",
    "hidden_size = 100\n",
    "encoder = EncoderRNN(input_size=len(kan_vocab), hidden_size=hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=len(eng_vocab)).to(device)\n",
    "optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 786989, 1045684,  997823, 1462632,  716675,  307981,  888138,  357443,\n",
      "         611688,  537984,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1237074613571167:  20%|██        | 1/5 [00:06<00:25,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 786989,  247647, 1067324, 1160534,   64319, 1120361,  157890,  827993,\n",
      "        1197435, 1192319,  311388,  537984,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327,\n",
      "         915327,  915327,  915327,  915327,  915327,  915327,  915327,  915327])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1237074613571167:  20%|██        | 1/5 [00:09<00:39,  9.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(eng_vocab)), eng_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(eng_batch))\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"training begin.\")\n",
    "\n",
    "# Training loop\n",
    "MODEL_SAVE_INTERVAL = 100 # save the model every so often\n",
    "losses = [] # average loss per epoch\n",
    "bar = trange(epochs, desc=f'')\n",
    "for epoch in bar:\n",
    "    epoch_loss = 0\n",
    "    for i, (kan_batch,eng_batch) in enumerate(dataloader): # TO-DO - Need to pad the data\n",
    "        eng_batch = torch.stack(eng_batch, dim=1)\n",
    "        kan_batch = torch.stack(kan_batch, dim=1)\n",
    "        print(kan_batch[0])\n",
    "\n",
    "        eng_batch = eng_batch.to(device)\n",
    "        kan_batch = kan_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(kan_batch)\n",
    "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor=eng_batch)\n",
    "\n",
    "        loss = criterion(decoder_outputs.view(-1, len(eng_vocab)), eng_batch.view(-1))\n",
    "        epoch_loss += (loss.item()/len(eng_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(epoch_loss)\n",
    "    bar.set_description(f'loss: {epoch_loss}')\n",
    "\n",
    "    if epoch % MODEL_SAVE_INTERVAL == 0:\n",
    "        torch.save(encoder.state_dict(), f\"encoder.pt\")\n",
    "        torch.save(decoder.state_dict(), f\"decoder.pt\")\n",
    "\n",
    "torch.save(encoder.state_dict(), f\"encoder_final.pt\")\n",
    "torch.save(decoder.state_dict(), f\"decoder_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_sentence):\n",
    "    # Preprocess input sentence\n",
    "    input_sequence = indic_tokenize.trivial_tokenize(input_sentence)\n",
    "    input_sequence.insert(0,\"<s>\")\n",
    "    input_sequence.append(\"</s>\")\n",
    "    vector =  [kan_word2index[word] for word in input_sequence]\n",
    "    input_sequence = pad_sequence(vector, kan_word2index['</s>'])\n",
    " \n",
    "    print(torch.tensor(input_sequence))\n",
    "    encoder_outputs, encoder_hidden = encoder(torch.tensor(input_sequence, device=device).view(-1, 1))\n",
    "    print(encoder_outputs)\n",
    "    decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
    " \n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    print(decoded_ids)\n",
    "\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "        if idx.item() == '</s>':\n",
    "            decoded_words.append('</s>')\n",
    "            break\n",
    "        decoded_words.append(idx.item())\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([786989, 457540, 492223, 207483, 537984, 915327, 915327, 915327, 915327,\n",
      "        915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327, 915327,\n",
      "        915327, 915327, 915327, 915327])\n",
      "tensor([[[-0.5865,  0.1517, -0.9291,  ..., -0.0988,  0.2387,  0.8958]],\n",
      "\n",
      "        [[-0.8166, -0.2477, -0.3299,  ...,  0.0995,  0.1259,  0.2477]],\n",
      "\n",
      "        [[ 0.0845, -0.5271, -0.1066,  ..., -0.0349,  0.1959,  0.3143]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3932, -0.7750, -0.9279,  ..., -0.0022, -0.2628,  0.9407]],\n",
      "\n",
      "        [[ 0.4274, -0.8297, -0.9421,  ..., -0.0028, -0.3001,  0.9565]],\n",
      "\n",
      "        [[ 0.4405, -0.8124, -0.9086,  ..., -0.0042, -0.3950,  0.9217]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[ 39465, 149941, 149941,  ..., 149941, 149941, 149941],\n",
      "        [ 39465, 149941, 149941,  ..., 149941, 149941, 149941],\n",
      "        [ 39465, 149941, 149941,  ..., 149941, 149941, 149941],\n",
      "        ...,\n",
      "        [ 39465, 149941, 149941,  ..., 149941, 149941, 149941],\n",
      "        [ 39465, 149941, 149941,  ..., 149941, 149941, 149941],\n",
      "        [ 39465, 149941, 149941,  ..., 149941, 149941, 149941]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 40 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು .\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m out\n",
      "Cell \u001b[0;32mIn[97], line 20\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m     18\u001b[0m decoded_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m decoded_ids:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     21\u001b[0m         decoded_words\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 40 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "out = translate_sentence('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು .')\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
